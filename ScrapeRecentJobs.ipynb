{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('ResumeAnalysis': conda)",
   "metadata": {
    "interpreter": {
     "hash": "26e0d223c6b652543b86b5d5c5ffe1a4b96a31867acf239f8943a157f7dfbe3b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages \n",
    "import csv\n",
    "from datetime import *\n",
    "import numpy\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<>:59: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n<>:59: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n<ipython-input-17-c846165030cb>:59: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n  for col in [col for col in indeed_0.columns if col is not 'posted']:\n"
     ]
    }
   ],
   "source": [
    "def get_job_results(url: str, info_id: str) -> list:\n",
    "    \"\"\"\"Return page results from job site\"\"\"\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    results = soup.find(id=info_id) \n",
    "    return results\n",
    "\n",
    "def collect_individual_search_info(site: str, individual_searches: list, total_results_frame, unique_results_frame, search_times: list, total_search_time: float, notes:str = None):\n",
    "    \"\"\"Return individual search info as one DataFrame\"\"\"\n",
    "    # Create framework for the individual search data. \n",
    "    individual_searches_frame = pd.DataFrame(individual_searches, columns=['search_location', 'search_title'])\n",
    "    individual_searches_frame['SearchTimes'] = search_times\n",
    "    individual_searches_frame['Site'] = site\n",
    "    individual_searches_frame['Date'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Collect the counts for the results before removing duplicates.\n",
    "    total_results = total_results_frame[['search_title', 'search_location', 'description']].groupby(['search_title', 'search_location'], as_index=False).count()\n",
    "\n",
    "    # Collect the counts for the results after removing duplicates.\n",
    "    unique_results = unique_results_frame[['search_title', 'search_location', 'description']].groupby(['search_title', 'search_location'], as_index=False).count()\n",
    "\n",
    "    # Add the Total Results count to the individual searches frame.\n",
    "    individual_searches_frame = pd.merge(\n",
    "        individual_searches_frame, \n",
    "        pd.DataFrame({\n",
    "            'search_title': total_results['search_title'], \n",
    "            'search_location': total_results['search_location'], \n",
    "            'TotalResults': total_results['description']\n",
    "            }),\n",
    "        on=['search_title', 'search_location'], \n",
    "        how='left'\n",
    "        )\n",
    "\n",
    "    # Add the Unique Results count to the individual searches frame.\n",
    "    individual_searches_frame = pd.merge(\n",
    "        individual_searches_frame, \n",
    "        pd.DataFrame({\n",
    "            'search_title': unique_results['search_title'], \n",
    "            'search_location': unique_results['search_location'], \n",
    "            'UniqueResults': unique_results['description']\n",
    "            }),\n",
    "            on=['search_title', 'search_location'], \n",
    "            how='left'\n",
    "            )\n",
    "    \n",
    "    # Where we didn't have a results count we must not have found anything, so replace the NaN with a 0.\n",
    "    for col in ['TotalResults', 'UniqueResults']:\n",
    "        individual_searches_frame[col][individual_searches_frame[col].isnull()] = 0\n",
    "\n",
    "    # Add the total search time to the data frame.\n",
    "    individual_searches_frame['TotalSearchTime'] = total_search_time\n",
    "    # Add any notes we have to the frame.\n",
    "    individual_searches_frame['Notes'] = notes\n",
    "    individual_searches_frame.to_csv('Diagnostics/ScraperPerformancebySearch.csv', mode='a', header=False)\n",
    "  \n",
    "\n",
    "def preprocess_jobs_recent(initial_data):\n",
    "    \"\"\"Clean up the jobs frame\"\"\"\n",
    "    for col in [col for col in indeed_0.columns if col is not 'posted']:\n",
    "        initial_data[col] = initial_data[col].str.replace('\\n', ' ')\n",
    "    processed_data = initial_data.drop_duplicates(subset=['link'], keep='first')\n",
    "    processed_data['description'] = processed_data['description'].str.replace(r\"([a-z])([A-Z])\", r\"\\1 \\.\\2\")\n",
    "    processed_data['description'] = processed_data['description'].str.replace(r' \\\\.', '')\n",
    "    return processed_data\n",
    "\n",
    "\n",
    "    \n",
    "def save_search_stats(site: str, total_returned_jobs: int, unique_jobs: int, search_time: float, total_searches: int, notes: str):\n",
    "    \"\"\"Write search statistics as new row in ScraperPerformance.csv\"\"\"\n",
    "    with open('Diagnostics/ScraperPerformance.csv', 'a+', newline='') as file:\n",
    "        write = csv.writer(file, delimiter=',')\n",
    "        notes = notes.replace(',', ';')\n",
    "        write.writerow([site, total_returned_jobs, unique_jobs, search_time, total_searches, date.today(), notes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####################\n",
    "# Data Frame Columns\n",
    "columns = [\n",
    "    'search_title', 'search_location', 'location', 'title', 'company', 'posted', 'salary', 'summary', 'link', 'description'\n",
    "    ]\n",
    "\n",
    "###############\n",
    "# Job titles\n",
    "prefixes = [\n",
    "    'entry level ', \n",
    "    # 'junior ', \n",
    "    # 'associate ', \n",
    "    ''\n",
    "    ]\n",
    "titles = [\n",
    "    'data', \n",
    "    # 'data analyst', \n",
    "    # 'data scientist',\n",
    "    # # 'data engineer',\n",
    "    # 'business analyst', \n",
    "    # 'financial analyst', \n",
    "    # 'macro analyst'\n",
    "    ]\n",
    "\n",
    "################\n",
    "# Job Locations\n",
    "search_locations = [\n",
    "    # 'New York, NY',\n",
    "    # # 'Newark, NJ', \n",
    "    # # 'Princeton, NJ', 'Jersey City, NJ', 'Trenton, NJ', \n",
    "    # # 'Bridgewater, NJ', 'Somerville, NJ', 'Summit, NJ', \n",
    "    # # 'Morristown, NJ', 'Edison, NJ', 'Metuchen, NJ', \n",
    "    # # 'Hackensack, NJ',\n",
    "    # # 'Philadelphia, PA', \n",
    "    # 'Stamford, CT', 'Greenwich, CT',\n",
    "    # 'Hartford, CT',\n",
    "    # 'New Haven, CT', \n",
    "    # 'Boston, MA', \n",
    "    # # 'New York', 'New Jersey', 'New Hampshire', 'Pennsylvania', 'Connecticut',\n",
    "    # # 'NY', 'NJ', 'NH', 'PA', \n",
    "    # 'CT', 'MA',\n",
    "    'remote'\n",
    "    ]\n",
    "\n",
    "\n",
    "n_searches = (len(titles) * len(prefixes)) * len(search_locations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "posted_date_cutoff = 7\n",
    "# posted_date_cutoff = input('What is our date cutoff for finding jobs? ')"
   ]
  },
  {
   "source": [
    "# Indeed"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Searching remote for entry level data positions \tSearch number 1 of 2\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-7e2b6fe1beaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[1;31m# Wait {sleep_time} seconds before hitting the page so they don't kick us out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m                 \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m                     \u001b[0mdescription_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_job_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescription_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'jobDescriptionText'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "################################\n",
    "################################\n",
    "#### Scrape Indeed for Jobs ####\n",
    "################# ###############\n",
    "################################\n",
    "\n",
    "# Four Fors: Because who needs a head hunter anyways?\n",
    "# 1. Search each location\n",
    "#   2. Search each job title\n",
    "#       3. Search each page of results \n",
    "#           4. Get each job's description\n",
    "# nlocations * ntitles * npages * njobdescriptions = niterations\n",
    "# \n",
    "# NOTE We sleep before hitting the page because if we over do it they won't give us the data\n",
    "# \n",
    "\n",
    "#######################\n",
    "# Change these fields #\n",
    "#######################\n",
    "\n",
    "#######################\n",
    "# URL Fields\n",
    "# How many pages per locale\n",
    "location_results = 100\n",
    "# How far out should we look?\n",
    "radius = 50\n",
    "\n",
    "\n",
    "\n",
    "########################\n",
    "# Performance Monitors #\n",
    "########################\n",
    "search_times_indeed = []\n",
    "individual_searches = []\n",
    "\n",
    "# Initialize these for performance monitoring\n",
    "ttm = True\n",
    "start_time = time.time()\n",
    "counter = 1\n",
    "\n",
    "# Array of jobs\n",
    "jobs_indeed = []\n",
    "visited_links = set()\n",
    "\n",
    "# Main two iterators for the search, \n",
    "for search_location in search_locations: \n",
    "    for search_title in [(s + t) for s in prefixes for t in titles]:\n",
    "\n",
    "       ####################################\n",
    "        # Monitoring Chunk\n",
    "        individual_searches.append([search_location, search_title])\n",
    "        if counter > 1: \n",
    "            search_time = time.time() - search_start\n",
    "            search_times_indeed.append(search_time)\n",
    "            if ttm:\n",
    "                print(f'The search took {search_time} seconds.\\n')\n",
    "        if ttm:\n",
    "            print(f'Searching {search_location} for {search_title} positions \\tSearch number {counter} of {n_searches}') \n",
    "        counter += 1\n",
    "        search_start = time.time()\n",
    "        #####################################\n",
    "\n",
    "        for nresults in range(0, location_results, 10):            \n",
    "            # Go to the url below and get the results from each page\n",
    "            s = search_location.replace(' ', '%20').replace(',', '%2C')\n",
    "            st = search_title.replace(' ', '%20')\n",
    "            url = f'https://www.indeed.com/jobs?q={st}+%2420%2C000&l={s}+&radius={radius}&start={str(nresults)}'\n",
    "            try:\n",
    "                results = get_job_results(url, 'resultsCol')\n",
    "            except:\n",
    "                continue\n",
    "            # If we have run out of postings move on\n",
    "            if results is None:\n",
    "                continue\n",
    "            job_elems = results.find_all('div', class_='jobsearch-SerpJobCard')\n",
    "            \n",
    "            ##############################\n",
    "            # FIND THE MOST RECENT JOB POSTS!!! \n",
    "            for job in job_elems:\n",
    "                # Make sure it's a valid post\n",
    "                title_elem = job.find('h2', class_='title')\n",
    "                if title_elem is None:  \n",
    "                    continue\n",
    "                else:\n",
    "                    title = title_elem.text.strip()\n",
    "\n",
    "                # Use the link to the job description to determine if it's\n",
    "                # a job post that we have already scraped during this run.\n",
    "                # If it is a job we've seen, continue on to the next one.\n",
    "                link = job.find('a')['href']\n",
    "                description_url = f'https://www.indeed.com{link}'\n",
    "                if description_url in visited_links:\n",
    "                    continue\n",
    "                else:\n",
    "                    visited_links.add(description_url)\n",
    "                \n",
    "                # The number of days ago it was posted is in the span with class date \n",
    "                when_elem = job.find('span', class_='date').text.strip()\n",
    "\n",
    "                # If it was posted over our limit we don't want it\n",
    "                if \"+\" in when_elem:\n",
    "                    continue\n",
    "\n",
    "                if when_elem[0] == 'A':\n",
    "                    try:\n",
    "                        n_days_ago = pd.to_numeric(when_elem[7:9])\n",
    "                        if n_days_ago > posted_date_cutoff:\n",
    "                            continue\n",
    "                    except ValueError:                \n",
    "                        # T for today and J for Just posted both get a value of zero.\n",
    "                        if (when_elem[7] == \"t\") | (when_elem[7] == \"j\"):\n",
    "                            n_days_ago = 0\n",
    "\n",
    "                        # If there's text that we haven't classified yet, let us know. \n",
    "                        # We are also setting the days since posting to -1 so that the entry \n",
    "                        # will be more visible when we go to sort the jobs.\n",
    "                        else:\n",
    "                            n_days_ago = -1\n",
    "                            print(f'We have come across unclassified text in the posting date, it looks like:\\t {when_elem}.')\n",
    "\n",
    "                else:\n",
    "\n",
    "                    try:\n",
    "                        n_days_ago = pd.to_numeric(when_elem[0:2])\n",
    "                        if n_days_ago > posted_date_cutoff:\n",
    "                            continue\n",
    "                    except ValueError:                \n",
    "                        # T for today and J for Just posted both get a value of zero.\n",
    "                        if (when_elem[0] == \"T\") | (when_elem[0] == \"J\"):\n",
    "                            n_days_ago = 0\n",
    "\n",
    "                        # If there's text that we haven't classified yet, let us know. \n",
    "                        # We are also setting the days since posting to -1 so that the entry \n",
    "                        # will be more visible when we go to sort the jobs.\n",
    "                        else:\n",
    "                            n_days_ago = -1\n",
    "                            print(f'We have come across unclassified text in the posting date, it looks like:\\t {when_elem}.')\n",
    "          \n",
    "                # Company:\n",
    "                company_elem = job.find('span', class_='company')\n",
    "                if company_elem is None:\n",
    "                    continue\n",
    "                else:\n",
    "                    company_elem = company_elem.text.strip()\n",
    "\n",
    "\n",
    "                #########################\n",
    "                #   Location: Where is the job\n",
    "                #   Salary: How much are they paying\n",
    "                #   Summary: What's the provided job summary\n",
    "                location_elem = job.find('span', class_='location')\n",
    "                salary_elem = job.find('span', class_='salary')\n",
    "                summary_elem = job.find('div', class_='summary')\n",
    "                 \n",
    "                \n",
    "                # If there isn't a location provided, denote that\n",
    "                if location_elem is not None:\n",
    "                    location_elem = location_elem.text.strip()\n",
    "\n",
    "                # If there isn't a salary provided, denote that\n",
    "                if salary_elem is not None:\n",
    "                    salary_elem = salary_elem.text.strip()\n",
    "                \n",
    "                 # If there isn't a job summary provided, denote that\n",
    "                if summary_elem is not None:\n",
    "                    summary_elem = summary_elem.text.strip()\n",
    "\n",
    "\n",
    "                ##################################\n",
    "                # Go to the Job Description Page #\n",
    "                ##################################\n",
    "                # Wait {sleep_time} seconds before hitting the page so they don't kick us out\n",
    "                # \n",
    "                time.sleep(2)\n",
    "                try:\n",
    "                    description_results = get_job_results(description_url, 'jobDescriptionText')\n",
    "                except:\n",
    "                    continue\n",
    "                # If we screwed up locating the description, denote that\n",
    "                if description_results is not None:\n",
    "                    description_text = description_results.text.strip()\n",
    "\n",
    "                # Create an array of info for this job posting\n",
    "                info = [\n",
    "                    search_title,\n",
    "                    search_location,\n",
    "                    location_elem, \n",
    "                    title,\n",
    "                    company_elem,\n",
    "                    when_elem,\n",
    "                    salary_elem,\n",
    "                    summary_elem,\n",
    "                    description_url,\n",
    "                    description_text\n",
    "                ]\n",
    "\n",
    "                # Append the job info array to the array of job posting arrays\n",
    "                jobs_indeed.append(info)\n",
    "\n",
    "# This adds the time for the final search.\n",
    "search_time = time.time() - search_start\n",
    "search_times_indeed.append(search_time)\n",
    "\n",
    "# Calculate the total search time.\n",
    "total_search_time = time.time() - start_time\n",
    "\n",
    "if ttm:\n",
    "     print(f'Indeed search completed in {total_search_time} seconds. {len(jobs_indeed)} results found (including duplicates).') \n",
    "\n",
    "# Convert the array of job arrays into a data frame.\n",
    "indeed_0 = pd.DataFrame(jobs_indeed, columns=columns)\n",
    "\n",
    "# Save raw data just in case\n",
    "indeed_0.to_csv('data/0rawdata/' + datetime.today().strftime(\"%Y-%m-%d\") + '_indeed.csv', index=False)\n",
    "indeed_0.to_csv('Z:/data/0rawdata/' + datetime.today().strftime(\"%Y-%m-%d\") + '_indeed.csv', index=False)\n",
    "\n",
    "# Filter and modify the original and put it in a new variable.\n",
    "indeed_1 = preprocess_jobs_recent(indeed_0)\n",
    "indeed_1.to_csv('data/' + datetime.today().strftime(\"%Y-%m-%d\") + '_indeed.csv', index=False)\n",
    "indeed_1.to_csv('Z:/data/' + datetime.today().strftime(\"%Y-%m-%d\") + '_indeed.csv', index=False)\n",
    "\n",
    "\n",
    "# Save the monitoring data into their respective csv's for later analysis.\n",
    "# collect_individual_search_info saves into a new csv which monitors performance of each individual search\n",
    "# collect_individual_search_info('Indeed', individual_searches, indeed_0, indeed_1, search_times_indeed, total_search_time, 'Recent Jobs Only')\n",
    "# save_search_stats('Indeed', len(jobs_indeed), len(indeed_1), total_search_time, n_searches, 'Recent Jobs Only')\n",
    "\n",
    "# Tell us when we're done!\n",
    "print(f'Returned {len(indeed_1)} unique entries from Indeed.')\n",
    "\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "# try:\n",
    "#     pd.to_numeric(a)\n",
    "# except ValueError:\n",
    "#     print('letters not numbers')\n",
    "# except:\n",
    "#     print('hi')\n",
    "len('Active 3 days ago.')\n",
    "\n",
    "phrase = 'Active today'\n",
    "\n",
    "phrase[7]"
   ]
  },
  {
   "source": [
    "# Monster"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Searching remote for entry level data positions \tSearch number 1 of 2\n",
      "The search took 2.044952154159546 seconds.\n",
      "\n",
      "Searching remote for data positions \tSearch number 2 of 2\n",
      "Monster search took 3.428002119064331 seconds to retreive 0 total results (including some duplicates).\n",
      "Returned 0 unique entries from Monster.com\n"
     ]
    }
   ],
   "source": [
    "\n",
    "jobs_monster = []\n",
    "visited_links = set()\n",
    "########################\n",
    "# Performance Monitors #\n",
    "########################\n",
    "search_times_monster = []\n",
    "individual_searches = []\n",
    "\n",
    "# Initialize these for performance monitoring\n",
    "ttm = True\n",
    "start_time = time.time()\n",
    "counter = 1\n",
    "\n",
    "for search_location in search_locations:\n",
    "    for search_title in [(s + t) for s in prefixes for t in titles]:\n",
    "\n",
    "        ####################################\n",
    "        # Monitoring Chunk\n",
    "        individual_searches.append([search_location, search_title])\n",
    "        if counter > 1: \n",
    "            search_time = time.time() - search_start\n",
    "            search_times_monster.append(search_time)\n",
    "            if ttm:\n",
    "                print(f'The search took {search_time} seconds.\\n')\n",
    "        if ttm:\n",
    "            print(f'Searching {search_location} for {search_title} positions \\tSearch number {counter} of {n_searches}') \n",
    "        counter += 1\n",
    "        search_start = time.time()\n",
    "        #####################################\n",
    "\n",
    "        sl = search_location.replace(' ', '-').replace(',', '__2C')\n",
    "        st = search_title.replace(' ', '-')\n",
    "        URL = f'https://www.monster.com/jobs/search/?q={st}&where={sl}&stpage=1&page=10'\n",
    "        try:\n",
    "            results = get_job_results(URL, 'ResultsContainer')\n",
    "            if results is None:\n",
    "                continue\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        job_elems = results.find_all('section', class_='card-content')\n",
    "        for job in job_elems:\n",
    "            # If the title or company isn't present, continue to the next one\n",
    "            title_elem = job.find('h2', class_='title')\n",
    "            if title_elem is None:\n",
    "                continue\n",
    "            else:\n",
    "                title = title_elem.text.strip()\n",
    "            \n",
    "            # If it was posted over our limit we don't want it\n",
    "                when_elem = job.find('time').text.strip()\n",
    "                if (when_elem[0] == \"P\") | (when_elem[0] == \"J\"):\n",
    "                    when_elem = 0\n",
    "                elif when_elem[0] == \"+\":\n",
    "                    continue\n",
    "                elif pd.to_numeric(when_elem[:2]) > posted_date_cutoff:\n",
    "                    continue\n",
    "                else:\n",
    "                    when_elem = pd.to_numeric(when_elem[:2])\n",
    "            \n",
    "            #####################\n",
    "            # Get the link for the page with the full job description,\n",
    "            # then check to see if we have already been to that page.\n",
    "            # If it's a link to a page we've been to already, \n",
    "            # continue on and get the next one.\n",
    "            link = job.find('a')['href']\n",
    "            if link in visited_links:\n",
    "                continue\n",
    "            else:\n",
    "                visited_links.add(link)\n",
    "\n",
    "           \n",
    "            # If the company element isn't present continue to the next\n",
    "            company_elem = job.find('div', class_='company')\n",
    "            if company_elem is not None:\n",
    "                company = company_elem.text.strip()\n",
    "            \n",
    "            # If there isn't a job location provided, indicate that\n",
    "            location_elem = job.find('div', class_='location')\n",
    "            if location_elem is not None:\n",
    "                location = location_elem.text.strip()\n",
    "            \n",
    "            # Wait some time, then go to the job description page and get the relevant info.\n",
    "            time.sleep(2)\n",
    "            description_results = get_job_results(link, 'main-content')\n",
    "            \n",
    "            # Get the job details (Salary, etc)\n",
    "            details_elem = description_results.findAll('div', class_='detail-row')\n",
    "            salary = None\n",
    "            job_type = None\n",
    "            for detail in details_elem:\n",
    "                dt = detail.text.strip()\n",
    "                if 'Salary' in dt:\n",
    "                    salary = dt\n",
    "                if 'Job Type' in dt:\n",
    "                    job_type = dt                    \n",
    "                # if when_elem is None and 'Posted' in dt:\n",
    "                    # posted = dt\n",
    "\n",
    "            # Get the job description.\n",
    "            description_elem = description_results.find('div', class_='job-description')\n",
    "            if description_elem is not None:\n",
    "                description = description_elem.text.strip()\n",
    "            else:\n",
    "                description = None\n",
    "            \n",
    "            # Bundle all the info\n",
    "            item = [\n",
    "                search_title,\n",
    "                search_location,\n",
    "                location,\n",
    "                title,\n",
    "                company,\n",
    "                when_elem,\n",
    "                salary,\n",
    "                job_type,\n",
    "                link,\n",
    "                description\n",
    "            ]\n",
    "            jobs_monster.append(item)\n",
    "\n",
    "# This adds the time for the final search.\n",
    "search_time = time.time() - search_start\n",
    "search_times_monster.append(search_time)\n",
    "\n",
    "total_search_time_monster = time.time() - start_time\n",
    "\n",
    "if ttm:\n",
    "     print(f'Monster search took {total_search_time_monster} seconds to retreive {len(jobs_monster)} total results (including some duplicates).')\n",
    "\n",
    "# Convert array of arrays into data frame\n",
    "monster_0 = pd.DataFrame(jobs_monster, columns=columns)\n",
    "# Save raw data just in case\n",
    "monster_0.to_csv('data/0rawdata/' + datetime.today().strftime(\"%Y-%m-%d\") + '_monster.csv', index=False)\n",
    "monster_0.to_csv('Z:/data/0rawdata/' + datetime.today().strftime(\"%Y-%m-%d\") + '_monster.csv', index=False)\n",
    "# filter and modify df going forward\n",
    "monster_1 = preprocess_jobs_recent(monster_0)\n",
    "monster_1.to_csv('data/' + datetime.today().strftime(\"%Y-%m-%d\") + '_monster.csv', index=False)\n",
    "monster_1.to_csv('Z:/data/' + datetime.today().strftime(\"%Y-%m-%d\") + '_monster.csv', index=False)\n",
    "# Save the monitoring data into their respective csv's for later analysis.\n",
    "# collect_individual_search_info saves into a new csv which monitors performance of each individual search\n",
    "# collect_individual_search_info('Monster', individual_searches, monster_0, monster_1, search_times_monster, total_search_time, 'Recent Jobs Only')\n",
    "# save_search_stats('Monster', len(monster_0), len(monster_1), total_search_time_monster, n_searches, 'Recent Jobs Only')\n",
    "\n",
    "print(f'Returned {len(monster_1)} unique entries from Monster.com')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'https://www.monster.com/jobs/search/?q=data&where=remote&stpage=1&page=10'"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "# df_0 = pd.concat([indeed_1, monster_1], ignore_index=True)\n",
    "# df_1 = df_0.drop_duplicates(subset=['description'])\n",
    "# # df = PreprocessJobs(df)\n",
    "# print('Both pre duplicate removal:', len(df_0), '\\n', 'Post duplicate removal:', len(df_1))\n",
    "\n",
    "# df = df_1\n",
    "results\n",
    "URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['description'] = df['description'].str.replace(',', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('data/' + datetime.today().strftime(\"%Y-%m-%d\") + '_recentjobs.tsv', sep='\\t', index=False)"
   ]
  }
 ]
}